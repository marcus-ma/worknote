大文件切割split
日常工作中需要对日志文件进行分析，当日志文件过大时，Linux中使用vim、cat、vim、grep、awk等这些工具对大文件日志进行分析将会成为梦魇。
split提供两种方式对文件进行切割：
根据行数切割，通过-l参数指定需要切割的行数
根据大小切割，通过-b参数指定需要切割的大小
demo1：如下以一个3.4G大小的日志文件做切割演示，每一个文件按照50000行做切割，指定文件名为split-line，-d参数以数字的方式显示
查看文件信息：ls -l 2020011702.log -h
按行切割:split -l 50000 -d --verbose 2020011702.log split-line
 #正在创建文件"split-line00"
 #正在创建文件"split-line01"
 #正在创建文件"split-line02"
 #正在创建文件"split-line03"
查看切割文件行数确认:wc -l split-line00
查看文件大小:ls -lh split-line0[0-9]
demo2:除了按照行数切割之外，split还支持通过文件大小进行切割，通过指定-b参数指定文件大小进行切割，文件大小单位支持K, M, G, T, P, E, Z，如下以切割为500M演示文件切割过程.
split -b 500M -d --verbose 2020011702.log split-size
ls -lh split-size0*

————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
如何分析查看线上机器的QPS
背景：线上有一个推荐服务，日志量比较大，想确认下是不是qps过高
问题：如何根据日志查看一个服务的qps

tail -f XXX.log，可以发现日志格式如下：
[8708-10 14:51:44 638 INFO ] [async task worker[61]] recommend.components.KeywordService[87] - cateid=252 pageNum=1 
[8708-10 14:51:44 666 INFO ] [async task worker[62]] recommend.components.KeywordService[87] - cateid=42205 
[8708-10 14:51:44 673 INFO ] [async task worker[0]] recommend.components.KeywordService[87] - cateid=29 pageNum=2 
[8708-10 14:51:44 677 INFO ] [async task worker[1]] recommend.components.KeywordService[87] - cateid=252 pageNum=3 

日志规范中，有一列“请求时间”，可以通过这个“请求时间”估算出服务的qps， 步骤如下：
（1）先找一条使得一个请求有且只有一行的日志，常用工具是grep，此例需要grep recommend.components.KeywordService，得出的结果，一个请求对应一行日志
（2）取出时间这一列，常用的工具是cut或者awk，这里介绍一下cut（大家到linux上去man一下）
    -d 参数，按照某个字符分隔
    -f 参数，取出分隔后的第几列
    这个例子中，按照“空格”分隔后，时间在第二列
进行1，2两步操作后，得到的结果为
    14:51:44
    14:51:44
    14:51:45
    14:51:45
    14:51:46
    14:51:46
（3）对结果进行去重，取计数，常用的工具是uniq，参数是-c

故，整个shell命令是：
命令：tail -f XXX.log | grep recommend.components.KeywordService | cut -d' ' -f2 | cut -d':' -f3 | uniq -c
说明：取增量 | 一个请求取一行 | 把时间截取出来 | 把秒数截取出来 | 去重取计数
得到的结果是
    136 43
    126 44
    115 45
    131 46
    132 47
可以看到，14:51:43，共有136条日志
    44，有126条日志
    45，有115条日志
    ...
结论，这个模块，单机的qps在120-130左右

需要注意的是，cut的目的是将请求日志时间戳中的“秒”取出来，所以随着请求日志格式的不同，cut的写法也要随机应变~


————————————————————————————————————————————————————————————————————————————————————————————————————————————————
查看服务端口是否正常

port_status(){

   #最少3个X，已达到随机的字符串
   temp_file=`mktemp port_status.XXX`

   #1:判断以来命令telnet是否存在
   [ ! -x /usr/bin/telnet ] && echo "telnet: not found command" && exit 1

   #2：此时端口 $1 IP $2 port
   (telnet $1 $2 <<EOF
    quit
    EOF
   ) &>$temp_file
   
   #分析文件的内容，判断结果
   if egrep "\^]" $temp_file &> /dev/null;then
      echo "$1 $2 is open"
   else
      echo " $1 $2 is close"
   fi

   rm -f $temp_file

}


————————————————————————————————————————————————————————————————————————————————————————————————————————————————

巧用curl命令定位性能瓶颈
新建一个txt文件(format.txt)，内容如下：
  DNS域名解析：	%{time_namelookup}\n
  TCP连接建立：	%{time_connect}\n
应用层连接建立：	%{time_appconnect}\n
服务器收到请求：	%{time_pretransfer}\n
服务器处理完成：	%{time_starttransfer}\n
	------------------\n
        共耗时：    %{time_total}\n
	
然后输入命令：curl -w "@format.txt" -o /dev/null -s "https://www.baidu.com"
就可以查看http请求各个环节的耗时情况，能够帮助我们排查和定位问题。
-o /dev/null的目的是把响应信息屏蔽掉，-s的目的是把进度信息屏蔽掉
https://curl.haxx.se/docs/manpage.html这里面，搜索-w，可以看到完整的参数，大家可以根据需求，添加到curl-time.txt文本中~，下面截取部分内容，供大家参考。
http_code The numerical response code that was found in the last retrieved HTTP(S) or FTP(s) transfer. In 7.18.2 the alias response_code was added to show the same info.

http_connect The numerical code that was found in the last response (from a proxy) to a curl CONNECT request. (Added in 7.12.4)

http_version The http version that was effectively used. (Added in 7.50.0)
————————————————————————————————————————————————————————————————————————————————————————————————————————————————

查看内存利用率：head -2 /proc/meminfo
MemTotal:        1019944 kB
MemFree:           89520 kB

>>head -2 /proc/meminfo | awk 'NR==1{t=$2}NR==2{f=$2;print (t-f)*100/t "%"}'
>>90.5532%


————————————————————————————————————————————————————————————————————————————————————————————————————————————————


查看pid进程所在的目录
pwdx pid

————————————————————————————————————————————————————————————————————————————————————————————————————————————————

crontab
首次使用crontab會提示使用什麽編輯器，最好选择vim.basic
crontab -l //显示用户的crontab文件的内容
crontab -e //编辑用户的crontab文件的内容
crontab -r //删除用户的crontab文件

————————————————————————————————————————————————————————————————————————————————————————————————————————————————

文件同步rsync
#使用-a标志会保留权限和其他目录属性，同时-v提供详细输出，以便您可以跟踪进度。
#exclude后面需要3个参数，第一个参数为要忽略的文件，第二个参数为要同步的文件(A)，第3个参数为被同步的文件(B)【即为将A同步到B】
rsync -avz --exclude='' /home/marcus/ng.log /home/marcus/rsy

————————————————————————————————————————————————————————————————————————————————————————————————————————————————


一：查看服务器状态
cpu利用率 ：vmstat
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
 r  b       swpd   free         buff  cache       si   so        bi    bo   in   cs us sy id wa st
 0  0 1549580 206024  56648 231628  170   48  1305   397    1    2  4  2 92  2  0	
主要看的是us(用户的使用)和sy(系统的使用)，id是剩余的，wa是磁盘IO使用率
获取脚本写法：使用awk工具(每行数据会以空格分隔开)
demo：vmstat | awk '{print $1}'
>>procs
     r
     0
此处就会获取出每行的第一个数据。我们想要获取的数据是第3行的第13列(us)、第14列(sy)、16(wa)的值
因此，脚本的写法是：
#!/bin/bash
function cpu(){
   #NR为代表当前的行号
   util=$(vmstat | awk '{if(NR==3)print $13+$14}')
   iowait=$(vmstat | awk '{if(NR==3)print $16}')
   echo "CPU - 使用率 : ${util}% ,等待磁盘IO响应使用率：${iowait}%"
}



内存：free -m
                    total         used        free      shared  buff/cache   available
Mem:           8101        6326        1551          17         223        1644
Swap:         24576         653       23922
主要看的是total和available的值
此处的数值都是以MB为单位，我们获取的时候要转换成G
总大小是要获取第2行的第2列，剩余可使用的是要获取第2行的最后一列，已使用的是第2行的第2列减去最后一列
因此脚本的写法是：
function memory() {
   #NF为最后一列
   total=$(free -m | awk '{if(NR==2)printf "%.1f",$2/1024}}')
   used=$(free -m | awk '{if(NR==2)printf "%.1f",($2-$NF)/1024}')
   available=$(free -m | awk '{if(NR==2)printf "%.1f",$NF/1024}')
   echo "内存 - 总大小：${total}G，已使用：${used}G，剩余：${available}"
}

磁盘利用率：df -h
Filesystem          Size    Used Avail  Use% Mounted on
/dev/sda3          237G  139G   99G  59% /
none                  237G  139G   99G  59% /dev
none                  237G  139G   99G  59% /run
none                  237G  139G   99G  59% /run/lock
none                  237G  139G   99G  59% /run/shm
/dev/sda1          237G  139G   99G  59% /run/user
C:                       237G  139G   99G  59% /mnt/c
上面的数据我们只需要看/dev/的分区
我们需要使用正则来匹配出以"/dev"开头的数据行，且有多个需要使用for循环来处理
因此脚本的写法是：
function disk(){
   fs=$(df -h |awk '/^\/dev/{print $1}')
   for p in $fs; do
        #awk的v参数为赋值模式，把当前循环的p值赋值变量q
        mounted=$(df -h | awk -v q=$p '$1==q{print $NF}')
        size=$(df -h |awk -v q=$p '$1==q{print $2}')
        used=$(df -h |awk -v q=$p '$1==q{print $3}')
        used_percent=$(df -h | awk -v q=$p '$1=q{print $5}')
        echo "硬盘 - 挂载点 : $mounted ，总大小：$size ，已使用：$used ，使用率：$used_percent"
   done
}


TCP连接数：netstat -antp
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp              0	0 localhost:48898		*:*	      LISTEN               12356
tcp              0	0 localhost:48896		*:*	      LISTEN               12336
tcp              0	0 localhost:48894		*:*	      ESTABLISHED     12756

 此处我们需要获取的是统计第6列State的值每个出现的次数
function tcp_status(){
   #a[$6]++代表把每个状态的次数都存在一个数组中，然后使用for循环把值输出
   summary=$(netstat -antp | awk '{a[$6]++}END{for(i in a)printf $i":"a[i]" "}')
   echo "TCP连接状态 - $summary"
}

二：找出占用CPU/内存过高的进程
#o参数代表自定义输出格式，pid为进程号，pcpu为占用cpu，pmem为占用内存，args为执行的命令
ps -eo pid,pcpu,pmem,args --sort=-pcpu | head -n 10
ps -eo pid,pcpu,pmem,args --sort=-pmem | head -n 10

三：磁盘告警
function alert_disk(){
   #OFS为输出一个分隔符，即为输出格式是$NF=$5\n,因为$5是带百分比的需要转整型判断
   USE_RATE_LIST = $(df -h | awk 'BEGIN{OFS="="}/^\/dev/{print $NF,int($5)}')
   for p in $USE_RATE_LIST;do
      #只要等號左邊的值
      PART_NAME = ${p%=*}
      #只保留等號右邊的值(連同等號和左邊的值一同去掉)
      USE_RATE=${p#*=}
      if [ $USE_RATE -ge 80 ];then
          echo "Warning: $PART_NAME Partition usage $USE_RATE%!"
      fi
   done 
}

四：mysql數據備份
#!bin/bash
#2020-02-11_15-19-58
DATE=$(date +%F_%H-%M-%S)
HOST=localhost
USER=root
PASS=123456
BACKUP_DIR=/data/db_backup
#s參數為去掉默认信息显示，e参数后面是要执行的命令，egrep后面的参数为过滤数据库掉默认的库名
DB_LIST=$(mysql -h$HOST -u$USER -p$PASS  -s  -e "show databases"; 2>/dev/null | egrep -v "Database|information_schema|mysql|performance_schema|sys")

for DB in $DB_list; do
    BACKUP_NAME=$BACKUP_DIR/${DB}_${DATE}.sql
    if ! mysqldump -h$HOST -u$USER -p$PASS -B $DB  >  $BACKUP_NAME 2>/dev/null; then
            echo "$BACKUP_NAME 備份失敗"
    fi
done
 

五：nginx日志分析
1：访问次数最多的前10个IP
awk '{a[$1]++}END{for(v in a)print v,a[v]}' ng.log | sort -k2 -nr | head -10
2：访问页面的次数
awk '{a[$7]++}END{for(v in a)print v,a[v]}' ng.log
3：访问页面和其状态码大于5次
awk '{a[$7" "$9]++}END{for(v in a){if(a[v]>5)print v,a[v]}}' ng.log
4：根据时间段访问的IP
awk '$4>=[25/Nov/2019:12:06:33" && $4<=[25/Nov/2019:21:06:33" {a[$1]++}END{for(v in a)print v,a[v]}' ng.log
5：统计访问UV
awk '{a[$1]++}END{print "UV:",length(a);for(v in a)print v,a[v]}' ng.log | sort -k2 -nr | head -10
6：统计访问PV
awk '{a[$7]++}END{print "PV:",length(a);for(v in a)print v,a[v]}' ng.log | sort -k2 -nr | head -10
7：按天分割日志信息
市面上用的工具是lograte,如果要用shell的话：
#ningx日志文件夹路径
LOG_DIR=/usr/local/nginx/logs
#前天的时间
YESTERDAY_TIME=$(date -d "yesterday" +%F)
#切割后的按年月放置的日志文件夹路径
LOG_MONTH_DIR=$LOG_DIR/$(date +"%Y-%m")
#日志的名字
LOG_FILE_LIST="access.log"

for LOG_FILE in $LOG_FILE_LIST; do
    #判断年月文件夹路径是否存在，不存在就创建
    [ ! -d $LOG_MOTH_DIR ]  && mkdir -p $LOG_MONTH_DIR
    #将当前的nginx日志移动到年月文件夹中并且按照时间重命名
    mv $LOG_DIR/$LOG_FILE $LOG_MONTH_DIR/${LOG_FILE}_${YESTERDAT_TIME}
done
#重新让nginx生成一份新的nginx日志(/var/run/nginx.pid的内容为nginx的进程号，通过该命令可向nginx发送一个信号让nginx重新生成日志)
kill -USR1 $(cat /var/run/nginx.pid)
8：将每分钟访问次数超过100的IP封掉
#时间输出到秒
date +"%F_%T"
#01/Dec/2019:21:19
date +%d/%b/%Y:%H:%M
脚本如下
#!/bin/bash
#当前分钟，格式转成nginx日志的时间格式
DATE=$(date +%d/%b/%Y:%H:%M)
ABNORMAL_IP=$(tail -n5000 access.log | grep $DATE | awk '{a[$1]++}END{for(i in a)if(a[i]>100)print i}')
for IP in $ABNORMAL_IP; do
     if [ $(iptables -vnL | grep -c "$IP") -eq 0 ]; then
	iptables -I INPUT -s $IP -j DROP
	echo "$(date +'%F_%T') $IP" >> /tmp/drop_ip.log
     fi
done


六：对指定目的创建、删除文件操作进行监听
使用到的工具包是：inotify-tools
sudo apt-get install inotify-tools或者sudo yum install inotify-tools
会用到的shell信息：
MON_DIR=/home/marcus/rsy
#m(持续监听)q(减少冗余信息，纸打印需要的信息)r(使用递归形式监视目录)e(制定要监视的时间，多个事件使用逗号隔开)
#--timefmt(时间格式)  --format(监听到的文件变化的信息:%w(发生事件的目录)%f(发生时间的文件)%e(发生的事件)%Xe(事件以"X"分隔)%T(时间格式))
#事件类型：access(访问|读取文件)modify(修改)attrib(属性)move(移动)create(创建)open(打开)close(关闭)delete(删除)
inotifywait  -mqr  --format %f -e create $MON_DIR | \
#上面的命令通过管道执行后会把文件名传到files
while read files; do
      echo "$(date +'%F %T') $files" >> file_mon.log
done
demo：监听日志ng.log，当发生修改，执行sidh.sh脚本
#!/bin/bash

MON_DIR=/home/marcus/ng.log
inotifywait  -mqr  --format %f -e modify $MON_DIR | \
while read files; do
      sh sidh.sh
      echo "$(date +'%F %T') $files" >> file_mon.log
done

